# -*- coding: utf-8 -*-
"""AgenticSentenceTransformer_Langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I9YBhv6iq8Vh-4-TRoPMTWQrJrk1_ZcY
"""

!pip install transformers #4.46.2
!pip install openai==0.28
!pip install gradio

from sentence_transformers import SentenceTransformer, util
from transformers import CLIPTokenizer
import torch
import pandas as pd
import numpy as np
import ast
import requests
from PIL import Image
from io import BytesIO
from IPython.display import display

from google.colab import userdata
import os
import openai
from langchain.chat_models import ChatOpenAI
from langchain.agents import create_react_agent, initialize_agent, Tool, ZeroShotAgent, load_tools, AgentExecutor, AgentType
from langchain.memory import ConversationBufferMemory

api_key = userdata.get('OA_API')
os.environ['OPENAI_API_KEY'] = api_key
openai.api_key = os.getenv('OPENAI_API_KEY')

np.random.seed(42)
llm_gpt4 = ChatOpenAI(model_name="gpt-4o-mini", openai_api_key= openai.api_key)
llm_gpt3 = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, openai_api_key=openai.api_key)

!pip show openai transformers

model_transformer = SentenceTransformer('clip-ViT-B-32')
tokenizer_clip = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

# ------------ Read DATA ------
data = pd.read_csv("/content/drive/MyDrive/GenAI@IISC_Group6/DB_Amazon_cellphone_features.csv")
data["ProductDetailsFineTuned"] = "-ASIN : " + data["parent_asin"] + "\n" + data["ProductDetailsFineTuned"]
features_list = data["ProductDetailsFineTuned"].tolist()
data.head()
# -----------------
review_data  = pd.read_csv("/content/drive/MyDrive/GenAI@IISC_Group6/DB_Amazon_cellphone_reviews.csv")
review_data["review_length"] = review_data["text"].str.split().str.len()
review_data["title_length"] = review_data["title"].str.split().str.len()
review_data = review_data[review_data["review_length"]>10]
review_data["review_text"] = "- " + review_data["text"]
review_data.head()

data[data['title'].str.contains("tracfone")]

# SENTENCE TRANSFORMER

# ------- Preprocessing Image URLS ---------
def extract_image_data(image_data):
  image_ls = (image_data[1:-1])
  large = ""
  thumbs = ""
  try:
    image_list = ast.literal_eval(image_ls)
    filtered_image_list = [item for item in image_list if item] #remove empty dict
    large = [item.get('large') for item in filtered_image_list if item.get('large')]
    if not large:
      print("empty large")
      thumbs = [item.get('thumb') for item in filtered_image_list if item.get('thumb')]
  except (SyntaxError, ValueError, AttributeError) as e:
    return ""

  return str(large) if large else str(thumbs) #return thumbs if large is empty

def extract_image_urls(image_string):
    try:
        image_urls = ""
        image_list = ast.literal_eval(image_string)
        if isinstance(image_list, list):
            for image_url in image_list:
                return image_url
                # image_urls = image_urls + image_url + ", "
            # return image_urls
        else:
            return ""
    except (SyntaxError, ValueError):
        print("Invalid image string format")
        return ""

# ------- Preprocessing Image URLS ---------


# ---------- TEXT IMAGE EMBEDDINGS ---------------

# Function to download and open an image from a URL
def get_image_from_url(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        image = Image.open(BytesIO(response.content))
        return image
    except (requests.exceptions.RequestException, IOError):
        return None  # Or handle the error as needed

def generate_image_embeddings(images):
  im_embeddings = []
  for image_url in images:
      image = get_image_from_url(image_url)
      if image:
          # Convert the NumPy array to a PyTorch tensor for model capability
          embeddings = model_transformer.encode(image)
          if isinstance(embeddings, (list, tuple)): # handles multi-image case
              im_embeddings.extend([torch.tensor(emb) for emb in embeddings])
          else:
              im_embeddings.append(torch.tensor(embeddings))
      else:
          print(f"Could not load image from URL: {image_url}")
  return im_embeddings

def truncate_text(text, max_length=74):
    tokens = tokenizer_clip.tokenize(text)
    if len(tokens) > max_length:
        tokens = tokens[:max_length - 1] + [tokenizer_clip.eos_token]  # Add EOS token if truncated
    return tokenizer_clip.convert_tokens_to_string(tokens)

def generate_text_embeddings(feature_data):
  feature_data = feature_data.apply(truncate_text) #VIT limitation
  text_embeddings = model_transformer.encode(feature_data.tolist())
  return text_embeddings

# Embeddings

def transformer_search_feature_text(query,embeddings):
  query_embedding = model_transformer.encode(query)
  results = util.semantic_search(query_embedding, embeddings, top_k=15)
  return results

def retreive_match_images(results, image_data):
  img_list = []
  images_filtered = []
  for result in results:
      try:
        pid=result['corpus_id']
        img_list.append(image_data[pid])
      except:
        img_list.append("")
  print("Image Links retrieved: " + str(img_list))
  for url in img_list:
    if(url):
      response = requests.get(url)
      response.raise_for_status()
      image = Image.open(BytesIO(response.content))
      images_filtered.append(image)
  return images_filtered

def transformer_search_feature_image(query, image_embeddings):
  query_embedding = model_transformer.encode(query)
  results = util.semantic_search(query_embedding, image_embeddings, top_k=15)
  return results

data['feature_extract'] = data['ProductDetails'].astype(str) + ", Price:" + data['price'].astype(str) + ", Average Rating:" + data['average_rating'].astype(str)
data['image_filtered'] = data['images'].apply(extract_image_data)
data['image_urls'] = data['image_filtered'].apply(extract_image_urls)
data = data[data['image_filtered'] != ""]
data = data[data['image_urls'] != ""]
product_data= data.reset_index(drop=True).to_dict(orient='index')
image_data = data["image_urls"]
feature_data = data['feature_extract']

image_embeddings = generate_image_embeddings(image_data)
text_embeddings = generate_text_embeddings(feature_data)

user_query = " 128gb cellphones"
text_results = transformer_search_feature_text(user_query, text_embeddings)
print("Matched Text results: for user query " + user_query + str(text_results))


image_query ="black cellphones"
image_results = transformer_search_feature_image(image_query, image_embeddings)
print("Matched Image results: for user query " + image_query + str(image_results))
image_dt = retreive_match_images(image_results[0], image_data)
[display(image) for image in image_dt]

!pip install langchain
!pip install langchain_community
from dotenv import load_dotenv,find_dotenv
load_dotenv(find_dotenv())

from langchain.chat_models import ChatOpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.docstore.document import  Document


def create_docs(results):
    docs = []
    for result in results[0]:
        pid = result['corpus_id']
        score = result['score']
        result_string = ''
        result_string += "feature_extract:" + str(product_data[pid]['feature_extract']) +  ';' + \
                          "Parent ASIN:" + str(product_data[pid]['parent_asin']) +  ';' + \
                          "Image URLS:" + str(product_data[pid]['image_urls'])
        doc = Document(page_content=result_string)
        doc.metadata['pid'] = str(pid)
        doc.metadata['score'] = score
        docs.append(doc)
    return docs

def recommend_by_product_features(query):
  results= transformer_search_feature_text(query, text_embeddings)
  docs = create_docs(results)
  response=chain({"input_documents": docs, "question": query},return_only_outputs=True)
  return response['output_text']

def recommend_by_image_color(query):
  results= transformer_search_feature_image(query, image_embeddings)
  docs = create_docs(results)
  response=chain({"input_documents": docs, "question": query},return_only_outputs=True)
  return response['output_text']
# Provide details like parent_asin, RAM, Storage, Camera, Processor, Screen Size, other important information also narrate pros and cons and at the end Recommend why is it good recommendedation given context in your interaction with the customer.

template = """
You are a E-commerce shopping assistant for cellphone products that wants to convert customers based on the information given.

Output should be in dictionary with key parent_asin, value : feature_extract also Narrate with good pros, cons and provide recommended reason based on user cotext. Do not return any extra characters.

Context: {context}

User question:{question}


Your response:"""
prompt = PromptTemplate.from_template(template)

chain = load_qa_chain(llm_gpt3, chain_type="stuff", prompt=prompt)

query="Provide good phones in black color with 512GB storage"
print(recommend_by_product_features(query))

# Attach transformer call to a tool and LLM
feature_transformer_tool = Tool(
    name = "FeatureTransformerTool",
    func = recommend_by_product_features,
    description = (
        "Recommends products based on features available"
        " Output should be in dictionary with key parent_asin, value : feature_extract also Narrate with good pros, cons and provide recommended reason based on user cotext. Do not return any extra characters."
    )
)

image_transformer_tool = Tool(
    name = "ImageTransformerTool",
    func = recommend_by_image_color,
    description = (
        "Recommend products based on image color. "
        " Output should be in dictionary with key parent_asin, value : feature_extract also Narrate with good pros, cons and provide recommended reason based on user cotext. Do not return any extra characters."
    )
)

recommendation_transformer_agent = initialize_agent(
    tools= [feature_transformer_tool, image_transformer_tool],
    llm= llm_gpt4,
    verbose=True,
    memory=ConversationBufferMemory(),
    handle_parsing_errors=True
)

query="Provide good phones in black color with 256GB storage" #512GB storage in black not found but providing recommendation ab Deep Purple similar
response = recommendation_transformer_agent.run(query)
print(response)

user_query = "Suggest good phones in black color with 12GB RAM"
recommendation_transformer_agent.run(user_query)

recommendation_transformer_agent

# prompt: gradio interface for accepting user input and generate product recommendation by invoking recommendation_transformer_agent.run(user_query) display in output textbox

import gradio as gr

def format_output(user_query):
    message = gr.update(value="Agents running in progress..")
    try:
      response = recommendation_transformer_agent.run(user_query)
      try:
          output_text = ""
          message = gr.update(value=response)
          return message

      except json.JSONDecodeError:
          message = gr.update(value="Invalid JSON format in agent response")
          return message
    except Exception as e:
        message = gr.update(value=f"An error occurred: {str(e)}")
        return message

iface = gr.Interface(
    fn=format_output,
    inputs=gr.Textbox(lines=2, placeholder="Enter your query here..."),
    outputs=gr.Textbox(), # Changed output to Textbox for plain text
    title="Product Recommendation by Sentence Transformer",
    description="Enter a query to get product recommendations.",
)

iface.launch()